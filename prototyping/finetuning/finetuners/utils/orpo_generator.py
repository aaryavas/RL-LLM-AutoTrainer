import pandas as pd
import logging
import torch
from typing import Optional, List, Tuple
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

logger = logging.getLogger(__name__)


class ORPODataGenerator:
    """
    Generates ORPO preference datasets from fine-tuned model outputs.
    
    This class creates chosen/rejected pairs where:
    - Chosen responses = ground truth labels from the original data
    - Rejected responses = generated by the fine-tuned model (which may be imperfect)
    
    This allows ORPO to learn to prefer the ground truth over model mistakes.
    """
    
    def __init__(
        self,
        model_path: Optional[str] = None,
        base_model_name: Optional[str] = None,
        device: str = "auto",
        load_in_4bit: bool = True,
    ):
        """
        Initialize the ORPO data generator.
        
        Args:
            model_path: Path to the fine-tuned model (adapter or merged)
            base_model_name: Base model name (required if model_path is an adapter)
            device: Device to use for generation
            load_in_4bit: Whether to load model in 4-bit quantization
        """
        self.model = None
        self.tokenizer = None
        self.model_path = model_path
        self.base_model_name = base_model_name
        self.device = device
        self.load_in_4bit = load_in_4bit
        
        if model_path:
            self._load_model()

    def _load_model(self) -> None:
        """Load the fine-tuned model for generating rejected responses."""
        logger.info(f"Loading model from {self.model_path}")
        
        # Check if this is an adapter or a merged model
        adapter_config_path = Path(self.model_path) / "adapter_config.json"
        is_adapter = adapter_config_path.exists()
        
        if is_adapter:
            # Load adapter on top of base model
            if not self.base_model_name:
                import json
                with open(adapter_config_path, "r") as f:
                    config = json.load(f)
                    self.base_model_name = config.get("base_model_name_or_path")
            
            if not self.base_model_name:
                raise ValueError("Base model name required for adapter loading")
            
            logger.info(f"Loading base model: {self.base_model_name}")
            
            if self.load_in_4bit:
                from transformers import BitsAndBytesConfig
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                )
                base_model = AutoModelForCausalLM.from_pretrained(
                    self.base_model_name,
                    quantization_config=bnb_config,
                    device_map=self.device,
                    trust_remote_code=True,
                )
            else:
                base_model = AutoModelForCausalLM.from_pretrained(
                    self.base_model_name,
                    torch_dtype=torch.float16,
                    device_map=self.device,
                    trust_remote_code=True,
                )
            
            logger.info(f"Loading adapter from {self.model_path}")
            self.model = PeftModel.from_pretrained(base_model, self.model_path)
            self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)
        else:
            # Load merged model directly
            logger.info(f"Loading merged model from {self.model_path}")
            if self.load_in_4bit:
                from transformers import BitsAndBytesConfig
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                )
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_path,
                    quantization_config=bnb_config,
                    device_map=self.device,
                    trust_remote_code=True,
                )
            else:
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_path,
                    torch_dtype=torch.float16,
                    device_map=self.device,
                    trust_remote_code=True,
                )
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
        
        # Ensure padding token is set
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        self.model.eval()
        logger.info("Model loaded successfully")

    def generate_rejected_responses(
        self,
        prompts: List[str],
        max_new_tokens: int = 256,
        temperature: float = 0.7,
        top_p: float = 0.9,
        batch_size: int = 4,
    ) -> List[str]:
        """
        Generate responses from the fine-tuned model to use as "rejected" examples.
        
        Args:
            prompts: List of input prompts
            max_new_tokens: Maximum tokens to generate
            temperature: Sampling temperature
            top_p: Top-p sampling parameter
            batch_size: Batch size for generation
            
        Returns:
            List of generated responses
        """
        if self.model is None:
            raise RuntimeError("Model not loaded. Initialize with model_path.")
        
        logger.info(f"Generating {len(prompts)} rejected responses...")
        
        generated_responses = []
        
        for i in range(0, len(prompts), batch_size):
            batch_prompts = prompts[i:i + batch_size]
            
            # Format prompts with chat template if available
            formatted_prompts = []
            for prompt in batch_prompts:
                if hasattr(self.tokenizer, "apply_chat_template"):
                    messages = [{"role": "user", "content": str(prompt)}]
                    formatted = self.tokenizer.apply_chat_template(
                        messages,
                        tokenize=False,
                        add_generation_prompt=True
                    )
                else:
                    formatted = f"User: {prompt}\nAssistant:"
                formatted_prompts.append(formatted)
            
            # Tokenize
            inputs = self.tokenizer(
                formatted_prompts,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512,
            ).to(self.model.device)
            
            # Generate
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                )
            
            # Decode responses (only the generated part)
            for j, output in enumerate(outputs):
                input_len = inputs["input_ids"][j].shape[0]
                response = self.tokenizer.decode(
                    output[input_len:],
                    skip_special_tokens=True
                ).strip()
                generated_responses.append(response)
            
            if (i + batch_size) % 20 == 0:
                logger.info(f"Generated {min(i + batch_size, len(prompts))}/{len(prompts)} responses")
        
        return generated_responses

    def create_orpo_dataset_from_synthetic(
        self,
        df: pd.DataFrame,
        text_column: str = "text",
        label_column: str = "label",
        generate_rejected: bool = True,
        max_new_tokens: int = 256,
        output_path: Optional[str] = None,
    ) -> pd.DataFrame:
        """
        Create ORPO dataset from synthetic data format (text/label columns).
        
        The synthetic data generator produces data with 'text' (prompt) and 'label' (ground truth).
        This function converts it to ORPO format:
        - prompt: The original text/prompt
        - chosen: The ground truth label
        - rejected: Generated response from the fine-tuned model (if generate_rejected=True)
        
        Args:
            df: DataFrame with text and label columns
            text_column: Name of the prompt column
            label_column: Name of the ground truth column
            generate_rejected: Whether to generate rejected responses using the model
            max_new_tokens: Max tokens for generation
            output_path: Optional path to save the ORPO dataset
            
        Returns:
            DataFrame with prompt, chosen, rejected columns
        """
        logger.info(f"Creating ORPO dataset from {len(df)} examples...")
        
        # Validate required columns
        if text_column not in df.columns:
            raise ValueError(f"Missing required column: {text_column}")
        if label_column not in df.columns:
            raise ValueError(f"Missing required column: {label_column}")
        
        # Extract prompts and chosen responses
        prompts = df[text_column].tolist()
        chosen_responses = df[label_column].tolist()
        
        # Generate rejected responses
        if generate_rejected:
            if self.model is None:
                raise RuntimeError(
                    "Model not loaded. Initialize with model_path to generate rejected responses."
                )
            rejected_responses = self.generate_rejected_responses(
                prompts,
                max_new_tokens=max_new_tokens,
            )
        else:
            # Use empty strings as placeholders (user must provide rejected_df)
            rejected_responses = [""] * len(prompts)
        
        # Create ORPO dataset
        orpo_data = []
        for i, (prompt, chosen, rejected) in enumerate(zip(prompts, chosen_responses, rejected_responses)):
            # Validate and clean data
            prompt = str(prompt).strip() if pd.notna(prompt) else ""
            chosen = str(chosen).strip() if pd.notna(chosen) else ""
            rejected = str(rejected).strip() if pd.notna(rejected) else ""
            
            if prompt and chosen and rejected:
                orpo_data.append({
                    "prompt": prompt,
                    "chosen": chosen,
                    "rejected": rejected
                })
            else:
                logger.warning(f"Row {i}: Incomplete data, skipping.")
        
        orpo_df = pd.DataFrame(orpo_data)
        logger.info(f"Created ORPO dataset with {len(orpo_df)} examples")
        
        # Save if path provided
        if output_path:
            orpo_df.to_csv(output_path, index=False)
            logger.info(f"Saved ORPO dataset to {output_path}")
        
        return orpo_df

    def create_orpo_dataset(
        self,
        df: pd.DataFrame,
        rejected_df: pd.DataFrame,
        prompt_col: str = "text",
        chosen_col: str = "label",
        rejected_col: str = "rejected_response"
    ) -> pd.DataFrame:
        """
        Creates the final ORPO dataset by combining chosen (ground truth) and rejected responses.
        
        This is the legacy method for when rejected responses are provided externally.

        Args:
            df: DataFrame containing the prompt and chosen response (ground truth).
            rejected_df: DataFrame containing the rejected responses. Assumed to be aligned with df.
            prompt_col: Name of the column containing the input prompt in df.
            chosen_col: Name of the column containing the chosen response in df.
            rejected_col: Name of the column containing the rejected response in rejected_df.

        Returns:
            DataFrame with 'prompt', 'chosen', and 'rejected' columns ready for ORPO training.
        """
        logger.info("Creating final ORPO dataset from provided rejected responses...")

        if len(df) != len(rejected_df):
            logger.warning(f"Input dataframe length ({len(df)}) does not match rejected dataframe length ({len(rejected_df)}). Truncating to minimum length.")
            min_len = min(len(df), len(rejected_df))
            df = df.iloc[:min_len]
            rejected_df = rejected_df.iloc[:min_len]

        orpo_data = []
        
        # Reset indices to ensure alignment if they are not aligned
        df = df.reset_index(drop=True)
        rejected_df = rejected_df.reset_index(drop=True)
        
        for idx in range(len(df)):
            prompt = df.loc[idx, prompt_col]
            chosen = df.loc[idx, chosen_col]
            rejected = rejected_df.loc[idx, rejected_col]
            
            # Basic validation
            if pd.notna(prompt) and pd.notna(chosen) and pd.notna(rejected):
                # Ensure strings
                prompt = str(prompt)
                chosen = str(chosen)
                rejected = str(rejected)
                
                if prompt and chosen and rejected:
                    orpo_data.append({
                        "prompt": prompt,
                        "chosen": chosen,
                        "rejected": rejected
                    })
            else:
                logger.warning(f"Row {idx}: Missing data. Skipping.")
        
        return pd.DataFrame(orpo_data)

